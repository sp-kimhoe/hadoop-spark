services:
  hadoop-service:
    build:
      context: ../hadoop
    container_name: hadoop-service
    environment:
      - CLUSTER_NAME=test
      - JAVA_HOME=/usr/lib/jvm/java-1.8-openjdk
    ports:
      - "9870:9870"  # HDFS web UI
      - "9000:9000"  # HDFS namenode
    volumes:
      - hadoop_service_data:/hadoop/dfs/name
      - hadoop_datanode:/data/hdfs/data
    networks:
      - hadoop_spark_network

  spark-master-service:
    build:
      context: ../spark
      # dockerfile: Dockerfile
    container_name: spark-master-service
    environment:
      - JAVA_HOME=/usr/local/java8
      - SPARK_HOME=/usr/local/spark
      - SPARK_MASTER_HOST=0.0.0.0
      - SPARK_MASTER_PORT=7077
    command: ["/bin/bash", "/usr/local/spark/start-spark.sh"]
    ports:
      - "8080:8080"                     # Spark Master UI
      - "7077:7077"                     # Spark Master Port
      - "4040:4040"                     # Spark Application UI
    volumes:
      - ..:/workspace:cached
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - hadoop-service
    networks:
      - hadoop_spark_network

  spark-worker-service:
    build:
      context: ../spark
      # dockerfile: Dockerfile
    container_name: spark-worker-service
    environment:
      - JAVA_HOME=/usr/local/java8
      - SPARK_HOME=/usr/local/spark
      - SPARK_MASTER_URL=spark://spark-master-service:7077  # Connect worker to master
    command: ["/bin/bash", "/usr/local/spark/start-worker.sh"]
    depends_on:
      - spark-master-service
    ports:
      - "8081:8081"
    networks:
      - hadoop_spark_network

volumes:
  hadoop_service_data:
    driver: local
  hadoop_datanode:
    driver: local

networks:
  hadoop_spark_network: