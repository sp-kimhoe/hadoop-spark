version: '3.8'

services:
  hadoop-service:
    build:
      context: ../hadoop
      dockerfile: Dockerfile
    container_name: hadoop-service
    environment:
      - JAVA_HOME=/usr/local/java8
      - HADOOP_HOME=/usr/local/hadoop
      - HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
      - HDFS_NAMENODE_USER=root
      - HDFS_DATANODE_USER=root
      - HDFS_SECONDARYNAMENODE_USER=root
      - YARN_RESOURCEMANAGER_USER=root
      - YARN_NODEMANAGER_USER=root
    ports:
      - "9870:9870"  # HDFS UI
      - "9000:9000"  # HDFS default FS
      - "8088:8088"  # ResourceManager UI
      - "8042:8042"  # NodeManager UI
    command: ["sh", "-c", "/usr/sbin/sshd && /usr/local/hadoop/start-hadoop.sh"]

  spark-master-service:
    build:
      context: ../spark
      dockerfile: Dockerfile
    container_name: spark-master-service
    environment:
      - JAVA_HOME=/usr/local/java8
      - SPARK_HOME=/usr/local/spark
      - SPARK_MASTER_HOST=0.0.0.0
      - SPARK_MASTER_PORT=7077
    command: ["/bin/bash", "/usr/local/spark/start-spark.sh"]
    ports:
      - "8080:8080"                     # Spark Master UI
      - "7077:7077"                     # Spark Master Port
      - "4040:4040"                     # Spark Application UI
    volumes:
      - ..:/workspace:cached
      - /var/run/docker.sock:/var/run/docker.sock

  spark-worker-service:
    build:
      context: ../spark
      dockerfile: Dockerfile
    container_name: spark-worker-service
    environment:
      - JAVA_HOME=/usr/local/java8
      - SPARK_HOME=/usr/local/spark
      - SPARK_MASTER_URL=spark://spark-master-service:7077  # Connect worker to master
    command: ["/bin/bash", "/usr/local/spark/start-worker.sh"]
    depends_on:
      - spark-master-service
    ports:
      - "8081:8081"                     # Spark Worker UI
